{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Rafa Literatura Brasileira .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMr+Ekdx+uAZwpWYm5VNzMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rafael-carvalho/brazilian-literature/blob/master/Rafa_Literatura_Brasileira_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBxJYzXIB1xR",
        "colab_type": "text"
      },
      "source": [
        "### Install required libraries and download the kaggle dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAkhx_AYaw-G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ce08da09-03e1-4332-9d70-276a48d460ec"
      },
      "source": [
        "\n",
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle/\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d rtatman/brazilian-portuguese-literature-corpus\n",
        "! unzip brazilian-portuguese-literature-corpus.zip\n",
        "! pip install nltk\n",
        "! pip install gensim\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.41.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.10)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Downloading brazilian-portuguese-literature-corpus.zip to /content\n",
            " 52% 9.00M/17.5M [00:00<00:00, 18.1MB/s]\n",
            "100% 17.5M/17.5M [00:00<00:00, 32.5MB/s]\n",
            "Archive:  brazilian-portuguese-literature-corpus.zip\n",
            "  inflating: Brazilian_Portugese_Corpus/A Alma do Lazaro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/A Condessa Vesper.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/A Danca dos Ossos.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/A Escrava Isaura.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/A Mao e a Luva.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/A Moreninha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/A Mortalha de Alzira.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/A Normalista.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/A Pata da Gazela.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/A Viuvinha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Adolfo Caminha/A Normalista.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Adolfo Caminha/Bom Crioulo.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Adolfo Caminha/No pais dos ianques.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Adolfo Caminha/Tentacao.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Alfarrabios_O Ermitao da Gloria.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/A Condessa Vesper.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/A Mortalha de Alzira.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/Casa de pensao.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/Demonios.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/Filomena Borges.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/Girandola de amores.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/Mattos, Malta ou Matta.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/O Cortico.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/O Esqueleto.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/O Homem.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/O Japao.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/O Livro de uma Sogra.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/O Mulato.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/O Touro Negro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Aluisio Azevedo/Uma Lagrima de Mulher.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Amor e Patria.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Antonica da Silva.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Ao Correr da Pena.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/As Minas de Prata.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/As Mulheres de Mantilha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/As Vitima-Algozes.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Bernardo Guimaraes/A Danca dos Ossos.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Bernardo Guimaraes/A Escrava Isaura.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Bernardo Guimaraes/Historias e Tradicoes da Provincia de Minas Gerais.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Bernardo Guimaraes/O Ermitao de Muquem.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Bernardo Guimaraes/O Garimpeiro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Bernardo Guimaraes/O Seminarista.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Bom Crioulo.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Casa Velha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Casa de pensao.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Cinco Minutos.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Contos Fluminenses.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Demonios.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Diva.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Dom Casmurro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Encarnacao.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Esau e Jaco.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Filomena Borges.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Girandola de amores.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Helena.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Historias Sem Data.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Historias da Meia-Noite.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Historias e Tradicoes da Provincia de Minas Gerais.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Iaia Garcia.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Iracema.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/A Moreninha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/Amor e Patria.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/Antonica da Silva.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/As Mulheres de Mantilha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/As Vitima-Algozes.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/Luneta Magica.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/Luxo e Vaidade.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/Memorias da Rua do Ouvidor.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/O Moco Loiro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/O Primo da California.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/Os Dois Amores.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/Os Romances da Semana.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/Remissao de Pecados.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/Romance de uma Velha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Joaquim Manuel de Macedo/Uma Pupila Rica.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/A Alma do Lazaro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/A Pata da Gazela.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/A Viuvinha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Alfarrabios_O Ermitao da Gloria.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Ao Correr da Pena.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/As Minas de Prata.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Cinco Minutos.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Diva.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Encarnacao.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Iracema.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Luciola.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Mae.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/O Demonio Familiar.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/O Garatuja.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/O Gaucho.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/O Guarani.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/O Rio de Janeiro Verso e Reverso.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/O Sertanejo.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/O que e o casamento.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Senhora.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Sonhos D'oro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Til.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Jose de Alencar/Ubirajara.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Luciola.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Luneta Magica.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Luxo e Vaidade.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/A Mao e a Luva.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Casa Velha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Contos Fluminenses.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Dom Casmurro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Esau e Jaco.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Helena.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Historias Sem Data.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Historias da Meia-Noite.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Iaia Garcia.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Memorial de Aires.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Memorias Postumas de Bras Cubas.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Paginas Recolhidas.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Papeis Avulsos.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Quincas Borba.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Reliquias de Casa Velha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Ressurreicao.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Machado de Assis/Varias Historias.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Mae.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Manuel Antonio de Almeida/Memorias de um Sargento de Milicias.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Mattos Malta ou Matta.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Memorial de Aires.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Memorias Postumas de Bras Cubas.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Memorias da Rua do Ouvidor.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Memorias de um Sargento de Milicias.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/No pais dos ianques.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Cortico.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Demonio Familiar.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Ermitao de Muquem.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Esqueleto.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Garatuja.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Garimpeiro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Gaucho.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Guarani.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Homem.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Japao.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Livro de uma Sogra.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Moco Loiro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Mulato.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Primo da California.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Rio de Janeiro Verso e Reverso.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Seminarista.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Sertanejo.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O Touro Negro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/O que e o casamento.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Os Dois Amores.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Os Romances da Semana.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Paginas Recolhidas.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Papeis Avulsos.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Quincas Borba.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Reliquias de Casa Velha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Remissao de Pecados.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Ressurreicao.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Romance de uma Velha.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Senhora.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Sonhos Doro.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Tentacao.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Til.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Ubirajara.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Uma Lagrima de Mulher.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Uma Pupila Rica.txt  \n",
            "  inflating: Brazilian_Portugese_Corpus/Varias Historias.txt  \n",
            "  inflating: guideToDocuments.csv    \n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.1.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.14.20)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.20 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.17.20)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.20->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.20->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_522wa3CIl0",
        "colab_type": "text"
      },
      "source": [
        "### Imports the libraries and downloads some text processing tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwO-hLNGbXsn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "19bec120-2eac-46ee-8007-cb56eb730c50"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import gensim\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words(\"portuguese\")\n",
        "stop_words.append('www.nead.unama.br')\n",
        "\n",
        "\n",
        "csv_path = '/content/guideToDocuments.csv'\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wTkcyL6CqDd",
        "colab_type": "text"
      },
      "source": [
        "### Pre Processing the files. Read the books in the appropriate encoding and processes the text to remove stopwords and break books into multiple X items"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGBjvzYXfUYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_book(path, path_prepend):\n",
        "  output = []\n",
        "  path = path.replace(\",\", \"\").replace(\"'\", \"\")\n",
        "  with open(f'{path_prepend}/{path}', \"r\", encoding='ISO8859_1', errors='ignore') as f:\n",
        "    output = f.readlines()\n",
        "  return output\n",
        "\n",
        "def read_book(book):\n",
        "  sentences = list()\n",
        "  for line in book:\n",
        "      clean_text = preprocess_text(line)\n",
        "      if clean_text:\n",
        "        sentences.append(clean_text)\n",
        "  \n",
        "  return sentences\n",
        "\n",
        "def group_lines_into_sequences(book_text, lines_per_sequence=10):\n",
        "  X = list()\n",
        "  sequence = list()\n",
        "  for i in range(len(book_text)):\n",
        "    sequence += book_text[i]\n",
        "    if i % lines_per_sequence == 0:\n",
        "      sequence = list()\n",
        "      X.append(sequence)\n",
        "  \n",
        "  output = list() \n",
        "  for sequence in X:\n",
        "    s = \" \"\n",
        "    output.append(s.join(sequence).strip())\n",
        "\n",
        "  return output\n",
        "\n",
        "\n",
        "def preprocess_text(input):\n",
        "  new_line = input.replace('www.nead.unama.br', \" \")\n",
        "  clean_text = gensim.utils.simple_preprocess(new_line)\n",
        "  clean_text = [i for i in clean_text if i not in stop_words]\n",
        "  return clean_text\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGPZdickChL2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "59752022-4e92-4cf4-a95d-fbe59e6dc055"
      },
      "source": [
        "data = list()\n",
        "for author, work in zip(df['Author'], df['Work']):\n",
        "  book = load_book(work, '/content/Brazilian_Portugese_Corpus')\n",
        "  lines = read_book(book)\n",
        "  sequences = group_lines_into_sequences(lines)\n",
        "  \n",
        "  for s in sequences:\n",
        "    data.append({\n",
        "        'author': author,\n",
        "        'work': work.replace('.txt', \"\"),\n",
        "        'sequence': s\n",
        "    })    \n",
        "\n",
        "\n",
        "dataframe = pd.DataFrame(data)\n",
        "dataframe.head()\n",
        "  "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>author</th>\n",
              "      <th>work</th>\n",
              "      <th>sequence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Adolfo Caminha</td>\n",
              "      <td>A Normalista</td>\n",
              "      <td>normalista nead núcleo educação distância av a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Adolfo Caminha</td>\n",
              "      <td>A Normalista</td>\n",
              "      <td>joão maciel mata gadelha conhecido fortaleza j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Adolfo Caminha</td>\n",
              "      <td>A Normalista</td>\n",
              "      <td>havia silêncio morno concentrado destacava rol...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Adolfo Caminha</td>\n",
              "      <td>A Normalista</td>\n",
              "      <td>risadinhas explodiam espaços gostosas indiscre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Adolfo Caminha</td>\n",
              "      <td>A Normalista</td>\n",
              "      <td>podem conferir disse erguendo risonho segunda ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           author  ...                                           sequence\n",
              "0  Adolfo Caminha  ...  normalista nead núcleo educação distância av a...\n",
              "1  Adolfo Caminha  ...  joão maciel mata gadelha conhecido fortaleza j...\n",
              "2  Adolfo Caminha  ...  havia silêncio morno concentrado destacava rol...\n",
              "3  Adolfo Caminha  ...  risadinhas explodiam espaços gostosas indiscre...\n",
              "4  Adolfo Caminha  ...  podem conferir disse erguendo risonho segunda ...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENKV-fEUCur3",
        "colab_type": "text"
      },
      "source": [
        "### One hot encoding for the author names. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeQ-e5K6zpsO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3bfa6e7b-3392-4f41-b78c-a11a835abd48"
      },
      "source": [
        "\n",
        "unique_values = dataframe['author'].unique()\n",
        "\n",
        "mapping = dict()\n",
        "for i, x in enumerate(unique_values):\n",
        "  mapping[x] = i\n",
        "\n",
        "print(mapping)\n",
        "dataframe['author_label'] = dataframe['author'].map(mapping)\n",
        "Y = dataframe['author_label'].values"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Adolfo Caminha': 0, 'Aluisio Azevedo': 1, 'Bernardo Guimaraes': 2, 'Joaquim Manuel de Macedo': 3, 'Jose de Alencar': 4, 'Machado de Assis': 5, 'Manuel Antonio de Almeida': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZyzgkx6DCeO",
        "colab_type": "text"
      },
      "source": [
        "### Splits the data into train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zmcdUfaFy9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_train, df_test, Ytrain, Ytest = train_test_split(dataframe['sequence'], Y, test_size=0.33)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-V_m8r_IDHOm",
        "colab_type": "text"
      },
      "source": [
        "### Imports all the Machine Learning libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBiMo8-oLO8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Dropout\n",
        "from tensorflow.keras.layers import LSTM, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZxxJK3JDL-v",
        "colab_type": "text"
      },
      "source": [
        "### Text preprocessing to get the data ready for the training. We use only the test data for the tokenization process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JxzwwAWK7io6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e3bc3f7-ed0c-4d6b-aa94-f30326096df0"
      },
      "source": [
        "# Convert sentences to sequences\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "embedding_dim = 16\n",
        "max_length = 200\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(df_train)\n",
        "sequences_train = tokenizer.texts_to_sequences(df_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(df_test)\n",
        "\n",
        "# get word -> integer mapping\n",
        "word2idx = tokenizer.word_index\n",
        "V = len(word2idx)\n",
        "print('Found %s unique tokens.' % V)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 73566 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bw_byBNDUt4",
        "colab_type": "text"
      },
      "source": [
        "### Pad the sentences to the fixed dimension"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z03zO_3LYeZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "abb179be-194e-46a3-aa1e-1ac79a2e6ec6"
      },
      "source": [
        "# pad sequences so that we get a N x T matrix\n",
        "data_train = pad_sequences(sequences_train, padding=padding_type, truncating=trunc_type)\n",
        "print('Shape of data train tensor:', data_train.shape)\n",
        "\n",
        "# get sequence length\n",
        "T = data_train.shape[1]\n",
        "data_test = pad_sequences(sequences_test, maxlen=T)\n",
        "print('Shape of data test tensor:', data_test.shape)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data train tensor: (22507, 116)\n",
            "Shape of data test tensor: (11087, 116)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfxjm6wMDaKK",
        "colab_type": "text"
      },
      "source": [
        "### Creates the actual model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzuR0qYnLff5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "5047a5d1-4ff2-4472-f8e0-3e60c2e81729"
      },
      "source": [
        "# Create the model\n",
        "\n",
        "# We get to choose embedding dimensionality\n",
        "D = 20\n",
        "\n",
        "# Hidden state dimensionality\n",
        "M = 20\n",
        "\n",
        "# Note: we actually want to the size of the embedding to (V + 1) x D,\n",
        "# because the first index starts from 1 and not 0.\n",
        "# Thus, if the final index of the embedding matrix is V,\n",
        "# then it actually must have size V + 1.\n",
        "\n",
        "i = Input(shape=(T,))\n",
        "x = Embedding(V + 1, D)(i)\n",
        "x = LSTM(M, return_sequences=True)(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dropout(rate=0.3)(x)\n",
        "x = Dense(len(unique_values), activation='softmax')(x)\n",
        "\n",
        "model = Model(i, x)\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 116)]             0         \n",
            "_________________________________________________________________\n",
            "embedding (Embedding)        (None, 116, 20)           1471340   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 116, 20)           3280      \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 20)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 7)                 147       \n",
            "=================================================================\n",
            "Total params: 1,474,767\n",
            "Trainable params: 1,474,767\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVRlKQnRDcxs",
        "colab_type": "text"
      },
      "source": [
        "### Compiles the model and trains it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvKN4DHiLjGq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "1d1bf155-6de8-42e4-e918-ed26dbb2e382"
      },
      "source": [
        "# Compile and fit\n",
        "model.compile(\n",
        "  loss='sparse_categorical_crossentropy',\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "\n",
        "\n",
        "print('Training model...')\n",
        "r = model.fit(\n",
        "  data_train,\n",
        "  Ytrain,\n",
        "  epochs=25,\n",
        "  validation_data=(data_test, Ytest),\n",
        "  callbacks=[]\n",
        ")\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training model...\n",
            "Epoch 1/25\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.8821 - accuracy: 0.6925 - val_loss: 0.5293 - val_accuracy: 0.8231\n",
            "Epoch 2/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.2887 - accuracy: 0.9077 - val_loss: 0.3475 - val_accuracy: 0.8914\n",
            "Epoch 3/25\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.1474 - accuracy: 0.9532 - val_loss: 0.4753 - val_accuracy: 0.8672\n",
            "Epoch 4/25\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.0952 - accuracy: 0.9709 - val_loss: 0.5393 - val_accuracy: 0.8461\n",
            "Epoch 5/25\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.0682 - accuracy: 0.9800 - val_loss: 0.3992 - val_accuracy: 0.9002\n",
            "Epoch 6/25\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.0697 - accuracy: 0.9796 - val_loss: 0.4368 - val_accuracy: 0.8821\n",
            "Epoch 7/25\n",
            "704/704 [==============================] - 17s 24ms/step - loss: 0.0576 - accuracy: 0.9832 - val_loss: 0.6882 - val_accuracy: 0.8587\n",
            "Epoch 8/25\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0434 - accuracy: 0.9868 - val_loss: 0.3809 - val_accuracy: 0.9036\n",
            "Epoch 9/25\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0570 - accuracy: 0.9826 - val_loss: 0.4210 - val_accuracy: 0.9084\n",
            "Epoch 10/25\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0471 - accuracy: 0.9855 - val_loss: 0.3763 - val_accuracy: 0.9039\n",
            "Epoch 11/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0429 - accuracy: 0.9869 - val_loss: 0.4662 - val_accuracy: 0.9001\n",
            "Epoch 12/25\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0363 - accuracy: 0.9881 - val_loss: 0.4069 - val_accuracy: 0.9045\n",
            "Epoch 13/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0328 - accuracy: 0.9896 - val_loss: 0.4723 - val_accuracy: 0.8880\n",
            "Epoch 14/25\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0339 - accuracy: 0.9895 - val_loss: 0.4718 - val_accuracy: 0.9036\n",
            "Epoch 15/25\n",
            "704/704 [==============================] - 17s 25ms/step - loss: 0.0321 - accuracy: 0.9898 - val_loss: 0.4077 - val_accuracy: 0.8859\n",
            "Epoch 16/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0476 - accuracy: 0.9849 - val_loss: 0.4756 - val_accuracy: 0.8950\n",
            "Epoch 17/25\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.0320 - accuracy: 0.9894 - val_loss: 0.5355 - val_accuracy: 0.8758\n",
            "Epoch 18/25\n",
            "704/704 [==============================] - 18s 26ms/step - loss: 0.0258 - accuracy: 0.9914 - val_loss: 0.5370 - val_accuracy: 0.8930\n",
            "Epoch 19/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0558 - accuracy: 0.9835 - val_loss: 0.5806 - val_accuracy: 0.8729\n",
            "Epoch 20/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0697 - accuracy: 0.9787 - val_loss: 0.6548 - val_accuracy: 0.8669\n",
            "Epoch 21/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0446 - accuracy: 0.9859 - val_loss: 0.7995 - val_accuracy: 0.8409\n",
            "Epoch 22/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0290 - accuracy: 0.9909 - val_loss: 0.5517 - val_accuracy: 0.8851\n",
            "Epoch 23/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0231 - accuracy: 0.9928 - val_loss: 0.7609 - val_accuracy: 0.8514\n",
            "Epoch 24/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0213 - accuracy: 0.9931 - val_loss: 0.6056 - val_accuracy: 0.8835\n",
            "Epoch 25/25\n",
            "704/704 [==============================] - 18s 25ms/step - loss: 0.0208 - accuracy: 0.9928 - val_loss: 0.8276 - val_accuracy: 0.8634\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me1IzTwKDqxq",
        "colab_type": "text"
      },
      "source": [
        "### Picks a random sample on the test set and verifies if the predicted value matches to the correct label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaXaz7hMldIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "sample = df_test.sample()\n",
        "val = sample.values[0]\n",
        "ind = sample.index[0]\n",
        "\n",
        "print(f'{unique_values[Y[ind]]}')\n",
        "print(val)\n",
        "pred = predict_raw_lines([val])\n",
        "\n",
        "print(unique_values[np.argmax(pred)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swCINouhDlB3",
        "colab_type": "text"
      },
      "source": [
        "### Required preparation for text that can be inputed by the user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udrELSpEX1Dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def prep(input):\n",
        "  new_line = input.replace('www.nead.unama.br', \" \")\n",
        "  \n",
        "  clean_text = gensim.utils.simple_preprocess(new_line)\n",
        "  \n",
        "  clean_text = [i for i in clean_text if i not in stop_words]\n",
        "  \n",
        "  return clean_text\n",
        "\n",
        "def predict_raw_lines(lines):\n",
        "  output = []\n",
        "  for l in lines:\n",
        "    new_line = prep(l)\n",
        "    s = \" \"\n",
        "    new_line = s.join(new_line).strip()\n",
        "    sequences = tokenizer.texts_to_sequences([new_line])\n",
        "    \n",
        "    padded = pad_sequences(sequences, maxlen=T, padding=padding_type, truncating=trunc_type)\n",
        "    output.append(padded)\n",
        "  \n",
        "  return model.predict(output)[0]\n",
        "\n",
        "\n",
        "print(unique_values)\n",
        "\n",
        "input = '''\n",
        "       Meu caro colega. ­ Acho-me seriamente embaraçado da maneira por que descreverei a\n",
        "visita que,  a qual já os nossos\n",
        "tiveram uma ligeira notícia neste mesmo jornal.'''\n",
        "\n",
        "pred = predict_raw_lines([input])\n",
        "print(np.asarray(pred))\n",
        "print(unique_values[np.argmax(pred)])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzoK1DgAFi9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_text = 'Olhe, continuou, acariciando-o sempre; n\\xE3o se meta com donzelas, entende?... S\\xE3o o diabo! Por d\\xE1 c\\xE1 aquela palha fica um homem em apuros! agora quanto \\xE0s outras, papo com elas! N\\xE3o mande nenhuma ao vig\\xE1rio, nem lhe doa a cabe\\xE7a, porque, no fim de contas, nas circunst\\xE2ncias de Dona Estela, \\xE9 at\\xE9 um grande servi\\xE7o que voc\\xEA lhe faz! Meu rico amiguinho, quando uma mulher j\\xE1 passou dos trinta e pilha a jeito um rapazito da sua idade, \\xE9 como se descobrisse ouro em p\\xF3! sabe-lhe a gaitas! Fique ent\\xE3o sabendo de que n\\xE3o \\xE9 s\\xF3 a ela que voc\\xEA faz o obs\\xE9quio, mas tamb\\xE9m ao marido: qua' #@param {type:\"string\"}\n",
        "pred = predict_raw_lines([input_text])\n",
        "print(np.asarray(pred))\n",
        "print(unique_values[np.argmax(pred)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaIqFznxF-8k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('literatura_brasileira.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}